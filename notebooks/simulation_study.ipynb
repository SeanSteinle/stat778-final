{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latter-beverage",
   "metadata": {},
   "source": [
    "# Interactive Simulation Study\n",
    "***Sean Steinle***\n",
    "\n",
    "In this notebook, we walk through both non-adaptive and adaptive versions of the Metropolis-Hastings algorithms. The core metric by which we compare the two algorithms is the rate of convergence, which is measured via a combination of visual inspection and autocorrelation. Because this is a simulation study, we seek to recover true parameters from randomly generated data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Non-Adaptive Metropolis-Hastings](#Non-Adaptive-Metropolis-Hastings)\n",
    "2. [Adaptive Metropolis-Hastings](#Adaptive-Metropolis-Hastings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demographic-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import uniform, normal, multivariate_normal, exponential, gamma\n",
    "import scipy as sc\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-egyptian",
   "metadata": {},
   "source": [
    "## Non-Adaptive Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "binary-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metropolis-hastings functions\n",
    "def sample(data: np.ndarray, N: int, B: int, start_theta: tuple, search_breadth: float=0.5, adaptive: bool=False):\n",
    "    \"\"\"Takes N samples via the Metropolis-Hastings algorithm, with B burn-in samples.\"\"\"\n",
    "    theta, Y = start_theta, []\n",
    "    for b in range(B): #burnin samples\n",
    "        if adaptive:\n",
    "            results = adaptive_step(data, theta, search_breadth, Y)\n",
    "            theta, Y = results['theta'], results['Y']\n",
    "        else:\n",
    "            results = step(data, theta, search_breadth)\n",
    "            theta = results['theta']\n",
    "    \n",
    "    samples = []\n",
    "    for n in range(N): #real samples\n",
    "        if adaptive:\n",
    "            results = adaptive_step(data, theta, search_breadth, Y)\n",
    "            theta, Y = results['theta'], results['Y']\n",
    "        else:\n",
    "            results = step(data, theta, search_breadth)\n",
    "            theta = results['theta']\n",
    "        samples.append(results)\n",
    "    return samples\n",
    "\n",
    "def step(data: np.ndarray, theta: tuple, search_breadth: float):\n",
    "    \"\"\"Takes one step in the Metropolis-Hastings algorithm by generating a new theta and comparing to a given theta.\"\"\"\n",
    "    theta_prime = sample_theta(theta, search_breadth) #sample a new set of parameters\n",
    "    acceptance_log_prob = calc_acceptance_prob(theta, theta_prime, data, search_breadth) #calculate the probability of acceptance\n",
    "    acceptance_prob = min(1,np.exp(acceptance_log_prob))\n",
    "    accepted = acceptance_prob >= uniform() #probabilistically determine acceptance\n",
    "    return {'accepted': accepted, 'acceptance_prob': acceptance_prob, 'theta': theta_prime if accepted else theta} #return results, update theta if samples accepted\n",
    "\n",
    "def sample_theta(theta: tuple, search_breadth: float):\n",
    "    \"\"\"Samples theta parameters--slope, intercept, and standard deviation.\"\"\"\n",
    "    a,b,sigma = theta\n",
    "    a,b = multivariate_normal([a,b], [[search_breadth**2,0],[0,search_breadth**2]])\n",
    "    sigma = gamma(sigma*search_breadth*500, 1/(search_breadth*500))\n",
    "    theta = a,b,sigma\n",
    "    return theta\n",
    "\n",
    "def calc_acceptance_prob(theta: tuple, theta_prime: tuple, data: np.ndarray, search_breadth: float):\n",
    "    \"\"\"Calculates acceptance log-probability by using a Bayesian linear model. Note: all terms are in log-values here,\n",
    "    so must be exponentiated before determining acceptance against u.\"\"\"    \n",
    "    theta_likelihood = likelihood(theta, data)\n",
    "    theta_prior = prior(theta)\n",
    "    \n",
    "    theta_p_likelihood = likelihood(theta_prime, data)\n",
    "    theta_p_prior = prior(theta_prime)\n",
    "    \n",
    "    pr = proposal_ratio(theta, theta_prime, search_breadth)\n",
    "    acceptance_ratio = theta_p_likelihood - theta_likelihood + theta_p_prior - theta_prior + pr\n",
    "    return acceptance_ratio\n",
    "\n",
    "#bayesian functions\n",
    "def likelihood(theta: tuple, data: np.ndarray):\n",
    "    \"\"\"Calculates the likelihood component of our linear model by measuring our parameters theta on the given data.\"\"\"\n",
    "    a,b,sigma = theta\n",
    "    x,y = data[0],data[1]\n",
    "    likelihoods = sc.stats.norm.logpdf(y, loc=a*x+b, scale=sigma) #find the likelihood of a sample given a normal distribution specified by our parameters and the data\n",
    "    return np.sum(likelihoods) #use log likelihood for stability\n",
    "\n",
    "def prior(theta: tuple):\n",
    "    \"\"\"Calculates the prior component of our linear model, specified \"\"\"\n",
    "    a,b,sigma = theta\n",
    "    ab_prior = sc.stats.multivariate_normal.logpdf([a,b], [0,0], [[100,0],[0,100]]) #cov defaults to 1\n",
    "    sigma_prob = sc.stats.gamma.logpdf(sigma, 1, 1)\n",
    "    return np.sum([ab_prior,sigma_prob])\n",
    "\n",
    "def proposal_ratio(theta: tuple, theta_prime: tuple, search_breadth: float):\n",
    "    \"\"\"Offsets bidirectionality of chained samples.\"\"\"\n",
    "    a,b,sigma = theta\n",
    "    a_p,b_p,sigma_p = theta_prime\n",
    "    old_given_new_ab = sc.stats.multivariate_normal.logpdf([a,b],[a_p,b_p],[[search_breadth**2,0],[0,search_breadth**2]])\n",
    "    old_given_new_sigma = sc.stats.gamma.logpdf(sigma, sigma_p*search_breadth*500, scale=1/(500*search_breadth))\n",
    "    old_given_new = old_given_new_ab + old_given_new_sigma\n",
    "    \n",
    "    new_given_old_ab = sc.stats.multivariate_normal.logpdf([a_p,b_p],[a,b],[[search_breadth**2,0],[0,search_breadth**2]])\n",
    "    new_given_old_sigma = sc.stats.gamma.logpdf(sigma_p, sigma*search_breadth*500, scale=1/(500*search_breadth))\n",
    "    new_given_old = new_given_old_ab - new_given_old_sigma\n",
    "\n",
    "    return old_given_new - new_given_old\n",
    "\n",
    "#viz functions\n",
    "def plot_convergence(df: pd.DataFrame):\n",
    "    \"\"\"Shows convergence of theta for a given results dataframe.\"\"\"\n",
    "    plt.plot(range(len(df)),df['a'], label='slope')\n",
    "    plt.plot(range(len(df)),df['b'], label='intercept')\n",
    "    plt.plot(range(len(df)),df['sigma'], label='variance')\n",
    "    plt.plot(range(len(df)),df['acceptance_prob'], label='accept_prob')\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend()\n",
    "    \n",
    "#highest level functions to coordinate runs\n",
    "def compare_convergences(starting_positions: list, data: np.ndarray, n_samples: int=20000, adaptive: bool=False, show_plots: bool=False, out_dir: str=''):\n",
    "    \"\"\"High-level function that coordinates many rounds of the Metropolis-Hastings algorithm,\n",
    "    each starting at a different position.\"\"\"\n",
    "    dfs = []\n",
    "    for start in starting_positions:\n",
    "        print(f\"working on {start}\")\n",
    "        samples = sample(data, n_samples, 0, start, 0.1, adaptive) #no burn-in, breadth of search should be 0.1 -- yields a 5% acceptance rate at convergence\n",
    "\n",
    "        #aggregate this run's results into dataframe\n",
    "        df = pd.DataFrame(samples)\n",
    "        df[['a','b','sigma']] = pd.DataFrame(df['theta'].tolist(), index=df.index)\n",
    "        df['start'] = str(start) #if this breaks just use an index and map\n",
    "        df = df.drop(['theta'], axis=1)\n",
    "        dfs.append(df) #append to the larger results df\n",
    "        \n",
    "    df = pd.concat(dfs)\n",
    "    \n",
    "    #estimation of parameter over time plots\n",
    "    for param in ['a','b','sigma']:\n",
    "        for start_val in df['start'].unique(): #there's definitely a more groupby-y way to do this but this will work!\n",
    "            param_from_start = df[df['start']==start_val][param]\n",
    "            plt.plot(range(len(param_from_start)), param_from_start, label=start_val)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel(param)\n",
    "        plt.title(f\"Evolution of {param.upper()} Parameter Estimate from Different Starts\")\n",
    "        plt.savefig(f'{out_dir}{param}_estimate_evolution_{\"adaptive\" if adaptive else \"nonadaptive\"}.jpg', bbox_inches='tight')\n",
    "        if show_plots: plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "    #autocorrelation plots (I use the ACF metric from statsmodels, see more here: https://github.com/statsmodels/statsmodels/blob/c22837f0632ae8890f56886460c429ebf356bd9b/statsmodels/tsa/stattools.py#L579)\n",
    "    lag = n_samples/10\n",
    "    for param in ['a','b','sigma']:\n",
    "        for start_val in df['start'].unique(): #there's definitely a more groupby-y way to do this but this will work!\n",
    "            param_from_start = acf(df[df['start']==start_val][param],nlags=lag)\n",
    "            plt.plot(range(len(param_from_start)), param_from_start, label=start_val)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel(f'Autocorrelation with Lag {lag}')\n",
    "        plt.title(f\"Autocorrelation for {param.upper()} Parameter Estimate from Different Starts\")\n",
    "        plt.savefig(f'{out_dir}{param}_autocorrelation_{\"adaptive\" if adaptive else \"nonadaptive\"}.jpg', bbox_inches='tight')\n",
    "        if show_plots: plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "    df.to_csv(f'{out_dir}results.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-juvenile",
   "metadata": {},
   "source": [
    "Taking a single step, you'll see that we get back a new sample with some acceptance probability and an acceptance decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electronic-rapid",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-b2748ebb8121>:28: RuntimeWarning: overflow encountered in exp\n",
      "  acceptance_prob = min(1,np.exp(acceptance_log_prob))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accepted': True,\n",
       " 'acceptance_prob': 1,\n",
       " 'theta': (3.91945434323867, 0.08912876193699548, 2.1302397034191203)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(10000)\n",
    "Y = np.random.normal(4,2,10000)\n",
    "data = np.vstack((X,Y))\n",
    "step(data,(4,0,2),0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-ethernet",
   "metadata": {},
   "source": [
    "Now let's try to recover some preset parameters using the algorithm. Here I set the true parameters to N(-13,19) and we start the algorithm at N(0x+0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-assessment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-b2748ebb8121>:28: RuntimeWarning: overflow encountered in exp\n",
      "  acceptance_prob = min(1,np.exp(acceptance_log_prob))\n"
     ]
    }
   ],
   "source": [
    "#if we start on the correct parameters (so theoretically no burn-in) how does our acceptance rate vary with breadth value?\n",
    "X = np.random.rand(10000)\n",
    "Y = np.random.normal(-13,19,10000)\n",
    "data = np.vstack((X,Y))\n",
    "samples = sample(data, 20000, 0, (0,0,1), 0.1)\n",
    "df = pd.DataFrame(samples)\n",
    "df[['a','b','sigma']] = pd.DataFrame(df['theta'].tolist(), index=df.index)\n",
    "df = df.drop(['theta'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted,rejected = df[df['accepted']],df[~df['accepted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Naive Acceptance Rate: {len(accepted)/len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_point = 2000\n",
    "df[convergence_point:]['accepted'].sum()/(len(df)-convergence_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-spokesman",
   "metadata": {},
   "source": [
    "We see that we converge to the correct parameters fairly quickly, in under 1,000 timesteps for all three parameters. If we're conservative and say we converged at timestep 2,000, our algorithm yields an 11% acceptance rate at our stationary distribution.\n",
    "\n",
    "Now let's do a full run starting at various distances from the true parameters. We should see that the chains starting at further distances from our original parameters take longer to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the data to our true parameters which the algorithm will recover\n",
    "mu,sigma = (50,10)\n",
    "X = np.random.rand(10000)\n",
    "Y = np.random.normal(mu,sigma,10000)\n",
    "data = np.vstack((X,Y))\n",
    "df = compare_convergences([(0,0,1),(0,10,2),(0,20,4),(0,30,6),(0,40,8),(0,50,10)], (50,10), 10000, False, True, '../results/simulation_study/notebook/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-encyclopedia",
   "metadata": {},
   "source": [
    "From these plots, it seems like the number of timesteps to converge is linear in the difference between the starting parameters and the true parameters. Let's see if we can do better with an adaptive Metropolis-Hastings algorithm!\n",
    "\n",
    "I should note that the autocorrelation plots are inconsistent in their effectiveness--they seem to work well for demonstrating the difference in convergence rates for sigma, but the water is muddier in describing our a and b parameters. For this reason, I prefer using visual inspection for elbow points rather than relying solely on crossing some arbitrary threshold of a convergence metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-assumption",
   "metadata": {},
   "source": [
    "## Adaptive Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adaptive metropolis-hastings functions (Python NEEDS multiple dispatch!)\n",
    "def adaptive_step(data: np.ndarray, theta: tuple, search_breadth: float, Y):\n",
    "    \"\"\"Takes one step in the Metropolis-Hastings algorithm by generating a new theta and comparing to a given theta.\"\"\"\n",
    "    theta_prime = adaptive_sample_theta(theta, search_breadth, Y) #sample a new set of parameters\n",
    "    acceptance_log_prob = adaptive_calc_acceptance_prob(theta, theta_prime, data, search_breadth, Y) #calculate the probability of acceptance\n",
    "    acceptance_prob = min(1,np.exp(acceptance_log_prob))\n",
    "    accepted = acceptance_prob >= uniform() #probabilistically determine acceptance\n",
    "    #TODO: must determine whether to append to Y, add to return dictionary.\n",
    "    return {'accepted': accepted, 'acceptance_prob': acceptance_prob, 'theta': theta_prime if accepted else theta, 'Y': Y} #return results, update theta if samples accepted\n",
    "\n",
    "def adaptive_sample_theta(theta: tuple, search_breadth: float, Y):\n",
    "    \"\"\"Samples theta parameters--slope, intercept, and standard deviation.\"\"\"\n",
    "    #TODO: should incorporate the history vector Y here.\n",
    "    a,b,sigma = theta\n",
    "    a,b = multivariate_normal([a,b], [[search_breadth**2,0],[0,search_breadth**2]])\n",
    "    sigma = gamma(sigma*search_breadth*500, 1/(search_breadth*500))\n",
    "    theta = a,b,sigma\n",
    "    return theta\n",
    "\n",
    "def adaptive_calc_acceptance_prob(theta: tuple, theta_prime: tuple, data: np.ndarray, search_breadth: float, Y):\n",
    "    \"\"\"Calculates acceptance log-probability by using a Bayesian linear model. Note: all terms are in log-values here,\n",
    "    so must be exponentiated before determining acceptance against u.\"\"\"\n",
    "    #TODO: should incorporate the history vector Y here.\n",
    "    \n",
    "    theta_likelihood = likelihood(theta, data)\n",
    "    theta_prior = prior(theta)\n",
    "    \n",
    "    theta_p_likelihood = likelihood(theta_prime, data)\n",
    "    theta_p_prior = prior(theta_prime)\n",
    "    \n",
    "    pr = proposal_ratio(theta, theta_prime, search_breadth)\n",
    "    acceptance_ratio = theta_p_likelihood - theta_likelihood + theta_p_prior - theta_prior + pr\n",
    "    return acceptance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(10000)\n",
    "Y = np.random.normal(-13,19,10000)\n",
    "data = np.vstack((X,Y))\n",
    "samples = sample(data, 10000, 0, (0,0,1), 0.1, True)\n",
    "df = pd.DataFrame(samples)\n",
    "df[['a','b','sigma']] = pd.DataFrame(df['theta'].tolist(), index=df.index)\n",
    "df = df.drop(['theta'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted,rejected = df[df['accepted']],df[~df['accepted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted[['a','b','sigma']][:100].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected[['a','b','sigma']][:100].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-alarm",
   "metadata": {},
   "source": [
    "We can see here that in the burn-in section, the accepted parameters are obviously closer to the value of the real parameters. So how to use this knowledge to push us to the true parameters faster?\n",
    "\n",
    "One idea is to use the averages of the accepted parameters as our prior. This should have the intended effect of speeding up convergence while also having the attractive quality of being an insignificant factor after convergence--that is, the likelihood overwhelms the prior at the stationary distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adaptive metropolis-hastings functions (Python NEEDS multiple dispatch!)\n",
    "def adaptive_step(data: np.ndarray, theta: tuple, search_breadth: float, Y: list):\n",
    "    \"\"\"Takes one step in the Metropolis-Hastings algorithm by generating a new theta and comparing to a given theta.\"\"\"\n",
    "    theta_prime = adaptive_sample_theta(theta, search_breadth, Y) #sample a new set of parameters\n",
    "    acceptance_log_prob = adaptive_calc_acceptance_prob(theta, theta_prime, data, search_breadth, Y) #calculate the probability of acceptance\n",
    "    acceptance_prob = min(1,np.exp(acceptance_log_prob))\n",
    "    accepted = acceptance_prob >= uniform() #probabilistically determine acceptance\n",
    "    if accepted:\n",
    "        theta = theta_prime\n",
    "        Y.append(theta_prime)\n",
    "    return {'accepted': accepted, \n",
    "            'acceptance_prob': acceptance_prob,\n",
    "            'theta': theta,\n",
    "            'Y': Y}\n",
    "\n",
    "def adaptive_sample_theta(theta: tuple, search_breadth: float, Y: list):\n",
    "    \"\"\"Samples theta parameters--slope, intercept, and standard deviation.\"\"\"\n",
    "    a,b,sigma = theta\n",
    "    a,b = multivariate_normal([a,b], [[search_breadth**2,0],[0,search_breadth**2]])\n",
    "    sigma = gamma(sigma*search_breadth*500, 1/(search_breadth*500))\n",
    "    theta = a,b,sigma\n",
    "    return theta\n",
    "\n",
    "def adaptive_calc_acceptance_prob(theta: tuple, theta_prime: tuple, data: np.ndarray, search_breadth: float, Y: list):\n",
    "    \"\"\"Calculates acceptance log-probability by using a Bayesian linear model. Note: all terms are in log-values here,\n",
    "    so must be exponentiated before determining acceptance against u.\"\"\"\n",
    "    avg_accepted_theta = (0,0,1)\n",
    "    if len(Y) > 0:\n",
    "        accepted_a,accepted_b,accepted_sigma = zip(*Y)\n",
    "        avg_accepted_theta = (np.mean(accepted_a),np.mean(accepted_b),np.mean(accepted_sigma))\n",
    "    \n",
    "    theta_likelihood = likelihood(theta, data)\n",
    "    theta_prior = prior(avg_accepted_theta) if len(Y) > 0 else prior(theta)\n",
    "    \n",
    "    theta_p_likelihood = likelihood(theta_prime, data)\n",
    "    theta_p_prior = prior(theta_prime)\n",
    "    \n",
    "    pr = proposal_ratio(theta, theta_prime, search_breadth)\n",
    "    acceptance_ratio = theta_p_likelihood - theta_likelihood + theta_p_prior - theta_prior + pr\n",
    "    return acceptance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(10000)\n",
    "Y = np.random.normal(50,10,10000)\n",
    "data = np.vstack((X,Y))\n",
    "samples = sample(data, 5, 0, (0,0,1), 0.1, True)\n",
    "adaptive_df = pd.DataFrame(samples)\n",
    "adaptive_df[['a','b','sigma']] = pd.DataFrame(adaptive_df['theta'].tolist(), index=adaptive_df.index)\n",
    "adaptive_df = adaptive_df.drop(['theta'], axis=1)\n",
    "adaptive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(10000)\n",
    "Y = np.random.normal(50,10,10000)\n",
    "data = np.vstack((X,Y))\n",
    "samples = sample(data, 10000, 0, (0,0,1), 0.1, True)\n",
    "adaptive_df = pd.DataFrame(samples)\n",
    "adaptive_df[['a','b','sigma']] = pd.DataFrame(adaptive_df['theta'].tolist(), index=adaptive_df.index)\n",
    "adaptive_df = adaptive_df.drop(['theta'], axis=1)\n",
    "adaptive_df['adaptive'] = True\n",
    "\n",
    "samples = sample(data, 10000, 0, (0,0,1), 0.1, False)\n",
    "nonadaptive_df = pd.DataFrame(samples)\n",
    "nonadaptive_df[['a','b','sigma']] = pd.DataFrame(nonadaptive_df['theta'].tolist(), index=nonadaptive_df.index)\n",
    "nonadaptive_df = nonadaptive_df.drop(['theta'], axis=1)\n",
    "nonadaptive_df['adaptive'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(adaptive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_df[5000:]['accepted'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(nonadaptive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonadaptive_df['accepted'][5000:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(10000),nonadaptive_df['accepted'].expanding().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(10000),adaptive_df['accepted'].expanding().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-stopping",
   "metadata": {},
   "source": [
    "Interestingly, we don't see much of a difference in the parameter estimations between the adaptive and non-adaptive versions. However, when we examine the cumulative average of the acceptance rate, we see where the algorithms differ. The adaptive algorithm actually gets to the same acceptance rate as convergence at around 1000 timesteps, but then it jumps up before slowly recovering to the convergence rate again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final test for adaptivity'    #setting the data to our true parameters which the algorithm will recover\n",
    "mu,sigma = (50,10)\n",
    "X = np.random.rand(10000)\n",
    "Y = np.random.normal(mu,sigma,10000)\n",
    "data = np.vstack((X,Y))\n",
    "df = compare_convergences([(0,0,1),(0,10,2),(0,20,4),(0,30,6),(0,40,8),(0,50,10)], (50,10), 10000, True, True, '../results/simulation_study/notebook/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the data to our true parameters which the algorithm will recover\n",
    "mu,sigma = (50,10)\n",
    "X = np.random.rand(10000)\n",
    "Y = np.random.normal(mu,sigma,10000)\n",
    "data = np.vstack((X,Y))\n",
    "df = compare_convergences([(0,0,1),(0,10,2),(0,20,4),(0,30,6),(0,40,8),(0,50,10)], (50,10), 10000, False, True, '../results/simulation_study/notebook/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
